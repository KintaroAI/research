# LLM Fundamentals

> Understanding the basics of Large Language Models through hands-on implementation.

## Objective

Build foundational understanding of transformer-based language models by implementing, training, and experimenting with them from scratch.

## Hypothesis

Hands-on implementation provides deeper understanding than reading papers alone. Starting with simple models and progressively adding complexity reveals which components are essential.

## Success Criteria

- [ ] Successfully train a GPT-style model locally
- [ ] Understand every component: embeddings, attention, FFN, layer norm
- [ ] Be able to modify and experiment with the architecture
- [ ] Document learnings that connect to our natural intelligence research

## Architecture: GPT-2 (Decoder-Only Transformer)

All experiments use a GPT-2-style decoder-only transformer. The architecture is defined in `dev/create_model.py` and executed in `dev/src/train_gpt2_fp32.cu`.

### Data Flow

```
Input token IDs: [tâ‚€, tâ‚, ..., t_{T-1}]    shape: (B, T)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Token Embedding (wte)     (V, C)       â”‚  look up each token â†’ dense vector
â”‚  + Position Embedding (wpe) (T, C)      â”‚  add learned position encoding
â”‚  = encoded                 (B, T, C)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Block Ã—L                   â”‚  repeated L times
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LayerNorm (ln1)                  â”‚  â”‚  normalize activations
â”‚  â”‚  â–¼                                â”‚  â”‚
â”‚  â”‚  Multi-Head Self-Attention        â”‚  â”‚
â”‚  â”‚  â”œâ”€ Linear â†’ Q, K, V   (Câ†’3C)    â”‚  â”‚  project to queries, keys, values
â”‚  â”‚  â”œâ”€ Split into NH heads (C/NH=HS) â”‚  â”‚  each head has dimension HS
â”‚  â”‚  â”œâ”€ Attention: softmax(QÂ·Káµ€/âˆšHS) â”‚  â”‚  compute attention weights (causal)
â”‚  â”‚  â”œâ”€ Weighted sum: att @ V         â”‚  â”‚  aggregate values
â”‚  â”‚  â””â”€ Linear projection  (Câ†’C)     â”‚  â”‚  project back
â”‚  â”‚  + residual connection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  â”‚  add input back (gradient highway)
â”‚  â”‚  â–¼                                â”‚  â”‚
â”‚  â”‚  LayerNorm (ln2)                  â”‚  â”‚  normalize again
â”‚  â”‚  â–¼                                â”‚  â”‚
â”‚  â”‚  Feed-Forward Network (MLP)       â”‚  â”‚
â”‚  â”‚  â”œâ”€ Linear   (C â†’ 4C)            â”‚  â”‚  expand
â”‚  â”‚  â”œâ”€ GELU activation               â”‚  â”‚  non-linearity
â”‚  â”‚  â””â”€ Linear   (4C â†’ C)            â”‚  â”‚  compress back
â”‚  â”‚  + residual connection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  â”‚  add input back
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final LayerNorm (lnf)     (B, T, C)    â”‚
â”‚  â–¼                                      â”‚
â”‚  Output Projection (wte)   (B, T, V)    â”‚  reuses token embedding (weight tying)
â”‚  â–¼                                      â”‚
â”‚  Softmax â†’ next-token probabilities     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configurable Hyperparameters

| Symbol | Parameter | What it controls | Effect of increasing |
|--------|-----------|-----------------|---------------------|
| `C` | `n_embd` (channels) | Width of all hidden representations | More expressive per-token features. Params scale as O(CÂ²) per layer. |
| `L` | `n_layer` | Depth â€” number of transformer blocks | More sequential computation steps. Each layer can refine representations further. Params scale linearly with L. |
| `NH` | `n_head` | Number of attention heads | More independent attention patterns. Each head sees C/NH dimensions. Total attention compute stays the same. |
| `T` | `block_size` (max_seq_len) | Maximum sequence length | Longer context window. Attention cost scales as O(TÂ²). Position embeddings grow as TÃ—C. |
| `V` | `vocab_size` | Number of distinct tokens | Larger vocabulary. Embedding table grows as VÃ—C. |

**Derived quantities:**
- **Head dimension** `HS = C / NH` â€” how many features each attention head can attend to
- **FFN hidden dim** = `4 Ã— C` â€” the MLP expansion factor (hardcoded to 4Ã—)
- **Total parameters** â‰ˆ `VÃ—C + TÃ—C + LÃ—(12CÂ² + 13C)` (dominated by the 12CÂ² from attention + MLP weights per layer)

### What Affects What

**Capacity / Expressiveness:**
- `C` (width) is the primary capacity dial. Doubling C roughly 4Ã— the parameters.
- `L` (depth) adds compositional ability â€” more layers = more sequential reasoning steps.
- `NH` (heads) controls diversity of attention patterns at fixed compute. More heads = more independent "feature detectors" but each is narrower (smaller HS).

**Compute / Memory:**
- Attention: O(B Ã— NH Ã— TÂ² Ã— HS) = O(B Ã— TÂ² Ã— C) â€” quadratic in sequence length
- MLP: O(B Ã— T Ã— 8CÂ²) per layer â€” quadratic in width
- Total: roughly O(B Ã— L Ã— T Ã— C Ã— (T + 8C))

**Training Dynamics:**
- Deeper models (large L) are harder to train â€” residual connections and layer norm are essential for gradient flow
- Wider models (large C) are easier to train but more memory-hungry
- More heads at fixed C means smaller HS, which can hurt if HS becomes too small for meaningful attention patterns

### Weight Tying

The output projection matrix that maps hidden states â†’ logits over vocabulary **reuses the token embedding matrix** (`wte`). This means the model uses the same representation for "what does this token look like as input" and "how likely is this token as output." This reduces parameters by VÃ—C and provides a useful inductive bias.

```python
self.lm_head.weight = self.transformer.wte.weight  # weight tying
```

In the CUDA code, this means the forward pass uses `params.wte` for both `encoder_forward` (input) and the final `matmul_forward` (output logits).

### Configurations Used

| Config | Layers | Heads | Embed | Seq Len | Vocab | Params | Use |
|--------|--------|-------|-------|---------|-------|--------|-----|
| Default (dev/) | 4 | 4 | 128 | 256 | 50,257 | ~7M | TinyStories baseline |
| Experiment 10 | 2 | 4 | 128 | 8 | 101 | ~400K | Modular arithmetic (grokking) |

### 16 Parameter Tensors (in checkpoint order)

| # | Name | Shape | Purpose |
|---|------|-------|---------|
| 1 | `wte` | (V_padded, C) | Token embeddings (also used as output projection) |
| 2 | `wpe` | (T, C) | Position embeddings |
| 3 | `ln1w` | (L, C) | Pre-attention LayerNorm weight (per layer) |
| 4 | `ln1b` | (L, C) | Pre-attention LayerNorm bias |
| 5 | `qkvw` | (L, 3C, C) | Attention QKV projection weight |
| 6 | `qkvb` | (L, 3C) | Attention QKV projection bias |
| 7 | `attprojw` | (L, C, C) | Attention output projection weight |
| 8 | `attprojb` | (L, C) | Attention output projection bias |
| 9 | `ln2w` | (L, C) | Pre-MLP LayerNorm weight |
| 10 | `ln2b` | (L, C) | Pre-MLP LayerNorm bias |
| 11 | `fcw` | (L, 4C, C) | MLP first linear weight (expand) |
| 12 | `fcb` | (L, 4C) | MLP first linear bias |
| 13 | `fcprojw` | (L, C, 4C) | MLP second linear weight (compress) |
| 14 | `fcprojb` | (L, C) | MLP second linear bias |
| 15 | `lnfw` | (C) | Final LayerNorm weight |
| 16 | `lnfb` | (C) | Final LayerNorm bias |

---

## Background

### Prior Work
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) â€” Vaswani et al., 2017 (original transformer)
- [GPT-2](https://openai.com/research/better-language-models) â€” Radford et al., 2019
- [nanoGPT](https://github.com/karpathy/nanoGPT) â€” Karpathy's minimal GPT implementation

### Key Concepts
- **Tokenization**: Converting text to integers
- **Embeddings**: Dense vector representations of tokens
- **Self-Attention**: Computing relationships between tokens (O(nÂ²) complexity)
- **Causal Masking**: Preventing tokens from seeing the future
- **Layer Normalization**: Stabilizing training
- **Residual Connections**: Enabling gradient flow

## Setup

### Requirements
- Hardware: GPU recommended (RTX 4090 available)
- Python 3.12+
- PyTorch 2.x with CUDA

### Installation
```bash
pip install torch numpy
```

## Progress

| Experiment | Date | Status | Key Finding |
|------------|------|--------|-------------|
| [00001-karpathy-gpt-local](./00001-karpathy-gpt-local/) | 2026-02-03 | ğŸ”¬ In Progress | Baseline implementation |
| [00010-delayed-generalization](./00010-delayed-generalization/) | 2026-02-07 | ğŸ”¬ In Progress | Reproduce delayed generalization on modular addition (mod 97) |

## Improvement Ideas: Encouraging Delayed Generalization

Based on [Generalization Beyond Overfitting on Small Algorithmic Datasets](https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf) (Power et al., ICLR 2021 MathAI Workshop):

- **Weight decay (AdamW, wd=1)** â€” the single most effective technique; dramatically improves data efficiency and generalization speed
- **Weight decay towards initialization** â€” variant where weights are regularized towards their initial values rather than the origin
- **Dropout (residual dropout = 0.1)** â€” helps but less impactful than weight decay
- **Gaussian weight noise (Ïƒ = 0.01)** â€” injecting noise into parameters during forward pass provides regularization
- **Increasing training data fraction** â€” more data consistently reduces the optimization steps needed to generalize (roughly exponential relationship)
- **Longer optimization budgets** â€” delayed generalization can take 100â€“1000Ã— more steps than memorization; training longer is essential
- **Learning rate tuning** â€” suboptimal hyperparameters severely limit generalization
- **SGD-based optimizers (Adam/AdamW) over full-batch methods** â€” mini-batch stochasticity helps find generalizing solutions

## Open Questions

- How does attention complexity relate to thalamic sorting?
- Can we visualize attention patterns as "correlation maps"?
- What's the minimum viable architecture for coherent text?
- Can we observe delayed generalization in our small-scale experiments?

## References

- Karpathy, A. "Let's build GPT: from scratch, in code, spelled out." YouTube, 2023.
- Vaswani, A., et al. "Attention Is All You Need." NeurIPS, 2017.
