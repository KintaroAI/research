--- /home/user/llm.c/train_gpt2_fp32.cu	2026-02-03 19:28:36.993535999 -0600
+++ /home/user/llm.c/train_gpt2_fp32_custom.cu	2026-02-03 19:53:04.496270800 -0600
@@ -1565,6 +1565,7 @@
     const char* train_data_pattern = "dev/data/tinyshakespeare/tiny_shakespeare_train.bin";
     const char* val_data_pattern = "dev/data/tinyshakespeare/tiny_shakespeare_val.bin";
     const char* output_log_file = NULL;
+    const char* model_path = "gpt2_124M.bin";
     int B = 4; // batch size
     int T = 1024; // sequence length max
     float learning_rate = 3e-4f;
@@ -1587,6 +1588,7 @@
         else if (argv[i][1] == 'm') { val_max_steps = atoi(argv[i+1]); }
         else if (argv[i][1] == 's') { sample_every = atoi(argv[i+1]); }
         else if (argv[i][1] == 'g') { genT = atoi(argv[i+1]); }
+        else if (argv[i][1] == 'e') { model_path = argv[i+1]; }
         else { error_usage(); }
     }
     printf("+-----------------------+----------------------------------------------------+\n");
@@ -1622,7 +1624,7 @@
 
     // build the GPT-2 model from a checkpoint
     GPT2 model;
-    gpt2_build_from_checkpoint(&model, "gpt2_124M.bin");
+    gpt2_build_from_checkpoint(&model, model_path);
     printf("| max_sequence_length T | %-50d |\n", model.config.max_seq_len);
     printf("| vocab_size V          | %-50d |\n", model.config.vocab_size);
     printf("| padded_vocab_size Vp  | %-50d |\n", model.config.padded_vocab_size);
