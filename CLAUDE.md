# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Research repository for AGI development through hands-on LLM implementation. Core work is GPT-2 training in raw CUDA/C with custom kernels, featuring banded sparsity experiments, task-position loss masking, and grokking studies.

## Repository Structure

- **`llm-fundamentals/dev/`** — Active development directory (work here)
- **`llm-fundamentals/NNNNN-experiment-name/`** — Archived experiment snapshots (copies of dev/ at completion)
- **`_templates/`** — README templates for experiments and topics

Experiments follow the naming convention `NNNNN-short-descriptive-name` (5-digit zero-padded sequential). When an experiment is complete, dev/ is snapshotted into a numbered directory with a comprehensive README documenting hypothesis, method, results, and analysis.

## Build & Test Commands

All commands run from `llm-fundamentals/dev/`:

```bash
# First-time setup
make setup          # Creates venv, installs Python deps (tiktoken, torch, etc.)
make data           # Downloads & tokenizes TinyStories dataset
make model          # Generates model.bin via create_model.py

# Build
make all            # Builds train and generate executables
make train          # Build training binary only
make generate       # Build inference binary only

# Test
make test           # Build and run the original fp32 correctness test
make test_all       # Build and run all 5 test suites
make test_banded_sparsity   # Build single test
./test_banded_sparsity      # Run single test after building

# Training
./train                           # Dense training (default)
./train -1 256 -2 256             # Banded sparsity on FC1+FC2
./train -p 4                      # Loss only at position 4 (for grokking)
./train -e model.bin -c out.bin   # Custom checkpoint in/out

# Inference
./generate -e checkpoint.bin -n 256 -p "Once upon a time"
```

Tests require `gpt2_124M.bin` and `gpt2_124M_debug_state.bin` (generated by `python train_gpt2.py --model gpt2 --write_tensors 1 --num_iterations 0`).

## Architecture

**Tech stack:** Python (PyTorch, tiktoken) for data prep and model creation; C/CUDA with cuBLAS for training and inference.

### CUDA Source (`llm-fundamentals/dev/src/`)

- **`train_gpt2_fp32.cu`** — Complete GPT-2 training: forward, backward, AdamW update, banded sparsity masks, task-position loss masking, checkpoint save/load, snapshot system for visualization. Uses `#ifndef TESTING` guard around `main()`.
- **`generate.cu`** — Inference-only forward pass with token sampling. Also has `#ifndef TESTING` guard.
- **`test_*.cu`** — Test suites use `#define TESTING` + `#include "train_gpt2_fp32.cu"` (or `generate.cu`) pattern to access internals without compiling `main()`.
- **`llmc/`** — Headers: `utils.h` (file I/O), `dataloader.h` (binary token loading), `tokenizer.h` (token decoding), `rand.h` (RNG).

### Checkpoint Format

Binary files with 256-int header (magic `20240326`, version `3`) followed by fp32 parameters. 16 parameter tensors stored contiguously: wte, wpe, ln1w, ln1b, qkvw, qkvb, attprojw, attprojb, ln2w, ln2b, fcw, fcb, fcprojw, fcprojb, lnfw, lnfb. Weight tying: wte serves as both input embedding and output projection.

### Key Features

- **Banded sparsity:** FC1/FC2 weight masks restrict connections to a band pattern. Masks applied during backward (gradient masking) and after optimizer update (weight enforcement). Controlled by `-1`/`-2` bandwidth flags (0 = dense).
- **Task-position masking:** `task_position` field restricts loss/gradients to a single sequence position per sample. Used for grokking experiments where only the answer token matters.
- **Memory-efficient backward:** Only 3 gradient activation buffers reused across layers, vs 21 forward activation buffers.

## Conventions

- No co-authored-by lines in commit messages
- No CI/CD or linting — pure research workflow
- Experiment READMEs serve as technical reports (hypothesis, method, results, analysis)
- Build tool is `nvcc` with flags `-O3 -lcublas -lm`
